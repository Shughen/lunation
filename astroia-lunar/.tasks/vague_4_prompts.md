# ğŸŒŠ Vague 4 : Testing & QA - Prompts Agents IA

**DurÃ©e totale** : 2h en parallÃ¨le (3 agents)
**PrÃ©requis** : Vague 3 terminÃ©e (routes API complÃ¨tes)
**Objectif** : Valider la qualitÃ©, la performance et l'intÃ©gration complÃ¨te du systÃ¨me V2

---

## ğŸ“‹ Vue d'ensemble Vague 4

| Agent | TÃ¢che | DurÃ©e | Fichiers principaux |
|-------|-------|-------|---------------------|
| Agent A | Task 3.4 : Tests E2E routes | 2h | `tests/test_lunar_routes_e2e.py` |
| Agent B | Task 4.1 : Tests intÃ©gration | 1h30 | `tests/test_lunar_integration.py` |
| Agent C | Task 4.2 : Benchmarks performance | 1h30 | `tests/test_lunar_performance.py`, `docs/PERFORMANCE_BENCHMARKS.md` |

---

## ğŸ¤– Agent A - Task 3.4 : Tests E2E Routes API (2h)

### ğŸ“Š Contexte

Tu es l'Agent A de la Vague 4. Ta mission est de crÃ©er des tests end-to-end complets pour valider que toutes les routes API de la Vague 3 fonctionnent correctement ensemble.

**PrÃ©requis complÃ©tÃ©s** :
- âœ… Vague 3 terminÃ©e : 3 routes API crÃ©Ã©es
  - GET /api/lunar-returns/current/report (Agent A, 3.1)
  - POST /api/lunar/interpretation/regenerate (Agent B, 3.2)
  - GET /api/lunar/interpretation/metadata (Agent C, 3.3)
- âœ… Service generator V2 opÃ©rationnel avec fallbacks
- âœ… Tests unitaires existants (514 passed)

### ğŸ¯ Objectif

CrÃ©er des tests E2E qui valident les scÃ©narios utilisateur complets, de bout en bout.

### ğŸ“ TÃ¢ches principales

#### 1. CrÃ©er fichier `tests/test_lunar_routes_e2e.py` (1h)

**Tests Ã  implÃ©menter** (minimum 10 tests) :

**a) ScÃ©nario complet gÃ©nÃ©ration â†’ metadata (3 tests)** :
```python
@pytest.mark.asyncio
async def test_e2e_generate_and_check_metadata(override_dependencies):
    """
    ScÃ©nario complet :
    1. GÃ©nÃ©rer une interprÃ©tation via GET /lunar-returns/current/report
    2. VÃ©rifier metadata via GET /interpretation/metadata
    3. Valider que total_interpretations a augmentÃ©
    """
    # Step 1: PremiÃ¨re lecture metadata (baseline)
    # Step 2: GÃ©nÃ©rer interprÃ©tation
    # Step 3: Seconde lecture metadata (vÃ©rifier +1)
    pass

@pytest.mark.asyncio
async def test_e2e_multiple_generations_models_used(override_dependencies):
    """
    GÃ©nÃ©rer plusieurs interprÃ©tations et vÃ©rifier que models_used
    reflÃ¨te correctement la distribution.
    """
    pass

@pytest.mark.asyncio
async def test_e2e_cached_rate_calculation(override_dependencies):
    """
    GÃ©nÃ©rer interprÃ©tations, attendre >1h, gÃ©nÃ©rer Ã  nouveau,
    vÃ©rifier que cached_rate est calculÃ© correctement.
    """
    pass
```

**b) ScÃ©nario rÃ©gÃ©nÃ©ration forcÃ©e (3 tests)** :
```python
@pytest.mark.asyncio
async def test_e2e_force_regenerate_bypasses_cache(override_dependencies):
    """
    1. GÃ©nÃ©rer interprÃ©tation normale
    2. Force regenerate avec POST /regenerate
    3. VÃ©rifier que metadata.forced = True
    4. VÃ©rifier que nouvelle version crÃ©Ã©e en DB
    """
    pass

@pytest.mark.asyncio
async def test_e2e_regenerate_updates_model_used(override_dependencies):
    """
    RÃ©gÃ©nÃ©rer plusieurs fois et vÃ©rifier que model_used
    dans metadata est mis Ã  jour.
    """
    pass

@pytest.mark.asyncio
async def test_e2e_regenerate_ownership_check(override_dependencies):
    """
    VÃ©rifier que user A ne peut pas rÃ©gÃ©nÃ©rer
    l'interprÃ©tation de user B (403 Forbidden).
    """
    pass
```

**c) Tests d'intÃ©gration metadata (2 tests)** :
```python
@pytest.mark.asyncio
async def test_e2e_metadata_cache_invalidation(override_dependencies):
    """
    1. GET metadata (mise en cache)
    2. GÃ©nÃ©rer nouvelle interprÃ©tation
    3. GET metadata (doit Ãªtre rafraÃ®chi si >10min)
    """
    pass

@pytest.mark.asyncio
async def test_e2e_metadata_empty_then_populated(override_dependencies):
    """
    Nouvel utilisateur :
    1. GET metadata â†’ total=0
    2. GÃ©nÃ©rer interprÃ©tation
    3. GET metadata â†’ total=1, models_used=[...]
    """
    pass
```

**d) Tests fallback hierarchy (2 tests)** :
```python
@pytest.mark.asyncio
async def test_e2e_fallback_to_template_on_claude_failure(override_dependencies):
    """
    Simuler Ã©chec Claude API, vÃ©rifier que fallback
    vers template fonctionne et metadata.source = 'db_template'.
    """
    pass

@pytest.mark.asyncio
async def test_e2e_metadata_reflects_fallback_source(override_dependencies):
    """
    GÃ©nÃ©rer avec diffÃ©rents fallbacks, vÃ©rifier que
    metadata reflÃ¨te correctement la source utilisÃ©e.
    """
    pass
```

#### 2. Validation et documentation (30min)

**a) ExÃ©cuter tous les tests E2E** :
```bash
pytest tests/test_lunar_routes_e2e.py -v
# Objectif : 10+ tests passed
```

**b) VÃ©rifier compatibilitÃ© avec suite existante** :
```bash
pytest -q
# Objectif : Aucune rÃ©gression (514+ passed)
```

**c) Documenter les scÃ©narios** :
Ajouter docstrings dÃ©taillÃ©s expliquant :
- Le scÃ©nario utilisateur
- Les Ã©tapes de validation
- Les assertions critiques

#### 3. Gestion des erreurs et edge cases (30min)

**Tests edge cases** :
```python
@pytest.mark.asyncio
async def test_e2e_concurrent_requests_same_user():
    """VÃ©rifier que requÃªtes concurrentes ne causent pas de race conditions."""
    pass

@pytest.mark.asyncio
async def test_e2e_metadata_after_db_failure():
    """VÃ©rifier fallback sur cache expirÃ© si DB inaccessible."""
    pass
```

### ğŸ“¦ Livrables

1. âœ… Fichier `tests/test_lunar_routes_e2e.py` avec 10+ tests
2. âœ… Tous tests E2E passent (10+ passed)
3. âœ… Aucune rÃ©gression dans suite existante (pytest -q)
4. âœ… Documentation claire des scÃ©narios dans docstrings

### ğŸ¯ CritÃ¨res de succÃ¨s

- [ ] 10+ tests E2E crÃ©Ã©s
- [ ] Tous tests passent sans erreur
- [ ] Code coverage E2E > 80% sur routes testÃ©es
- [ ] Aucune rÃ©gression dans suite existante
- [ ] Docstrings complÃ¨tes et claires

### ğŸ“š RÃ©fÃ©rences

**Fichiers Ã  Ã©tudier** :
- `routes/lunar.py` : Endpoint GET /metadata
- `routes/lunar_returns.py` : Endpoints avec metadata V2
- `services/lunar_interpretation_generator.py` : Service gÃ©nÃ©ration
- `tests/test_auth_protected_routes.py` : Exemples tests auth
- `tests/conftest.py` : Fixtures disponibles

**Pattern de test E2E** :
```python
@pytest.mark.asyncio
async def test_e2e_scenario_name(override_dependencies):
    async with AsyncClient(app=app, base_url="http://test") as client:
        # Step 1: Setup
        # Step 2: Action
        # Step 3: Validation
        pass
```

---

## ğŸ¤– Agent B - Task 4.1 : Tests IntÃ©gration Service â†’ DB (1h30)

### ğŸ“Š Contexte

Tu es l'Agent B de la Vague 4. Ta mission est de crÃ©er des tests d'intÃ©gration qui valident l'interaction entre le service generator et la base de donnÃ©es, sans passer par les routes HTTP.

**PrÃ©requis complÃ©tÃ©s** :
- âœ… Service `lunar_interpretation_generator.py` enrichi (Vague 1, 2.1)
- âœ… Tests unitaires generator (33 tests, Vague 2, 2.4)
- âœ… ModÃ¨les DB LunarInterpretation + LunarInterpretationTemplate

### ğŸ¯ Objectif

Valider que le service generator interagit correctement avec la base de donnÃ©es dans tous les scÃ©narios (cache hit, miss, fallback, erreurs).

### ğŸ“ TÃ¢ches principales

#### 1. CrÃ©er fichier `tests/test_lunar_integration.py` (1h)

**Tests Ã  implÃ©menter** (minimum 8 tests) :

**a) Tests cache DB temporelle (3 tests)** :
```python
@pytest.mark.asyncio
async def test_integration_cache_hit_from_db():
    """
    1. CrÃ©er LunarInterpretation en DB
    2. Appeler generate_or_get_interpretation()
    3. VÃ©rifier cache hit (pas de nouvelle gÃ©nÃ©ration)
    4. VÃ©rifier source = 'db_temporal'
    """
    pass

@pytest.mark.asyncio
async def test_integration_idempotence_constraint():
    """
    VÃ©rifier que contrainte UNIQUE (lunar_return_id, subject, lang, version)
    empÃªche les doublons.
    """
    pass

@pytest.mark.asyncio
async def test_integration_cache_miss_creates_new():
    """
    1. DB vide
    2. Appeler generate_or_get_interpretation()
    3. VÃ©rifier nouvelle entrÃ©e crÃ©Ã©e en DB
    4. VÃ©rifier input_json + output_text sauvegardÃ©s
    """
    pass
```

**b) Tests fallback templates (2 tests)** :
```python
@pytest.mark.asyncio
async def test_integration_fallback_to_template():
    """
    1. Simuler Ã©chec Claude (timeout)
    2. VÃ©rifier fallback vers LunarInterpretationTemplate
    3. VÃ©rifier source = 'db_template'
    """
    pass

@pytest.mark.asyncio
async def test_integration_template_lookup():
    """
    CrÃ©er template en DB avec combinaison spÃ©cifique,
    vÃ©rifier qu'il est trouvÃ© et utilisÃ© correctement.
    """
    pass
```

**c) Tests metadata persistence (2 tests)** :
```python
@pytest.mark.asyncio
async def test_integration_metadata_saved_to_db():
    """
    GÃ©nÃ©rer interprÃ©tation, vÃ©rifier que model_used,
    created_at, version sont sauvegardÃ©s en DB.
    """
    pass

@pytest.mark.asyncio
async def test_integration_weekly_advice_persistence():
    """
    VÃ©rifier que weekly_advice (JSONB) est sauvegardÃ©
    et rÃ©cupÃ©rÃ© correctement de la DB.
    """
    pass
```

**d) Tests force_regenerate (1 test)** :
```python
@pytest.mark.asyncio
async def test_integration_force_regenerate_bypasses_cache():
    """
    1. Cache hit normal
    2. force_regenerate=True â†’ nouvelle gÃ©nÃ©ration
    3. VÃ©rifier nouvelle entrÃ©e DB crÃ©Ã©e
    """
    pass
```

#### 2. Tests avec DB rÃ©elle (optionnel mais recommandÃ©) (15min)

Si possible, crÃ©er quelques tests avec connexion DB rÃ©elle Supabase :

```python
@pytest.mark.skipif(not DB_AVAILABLE, reason="DB Supabase non accessible")
@pytest.mark.asyncio
async def test_integration_real_db_save_and_retrieve():
    """
    Test avec vraie DB Supabase :
    1. Sauvegarder interprÃ©tation
    2. RÃ©cupÃ©rer depuis DB
    3. VÃ©rifier intÃ©gritÃ© donnÃ©es
    4. Cleanup
    """
    pass
```

#### 3. Validation et documentation (15min)

**a) ExÃ©cuter tests intÃ©gration** :
```bash
pytest tests/test_lunar_integration.py -v
# Objectif : 8+ tests passed
```

**b) VÃ©rifier compatibilitÃ©** :
```bash
pytest -q
# Objectif : Aucune rÃ©gression
```

**c) Documenter patterns d'intÃ©gration** :
Ajouter commentaires expliquant :
- Setup DB pour tests
- Mocking vs vraie DB
- Cleanup aprÃ¨s tests

### ğŸ“¦ Livrables

1. âœ… Fichier `tests/test_lunar_integration.py` avec 8+ tests
2. âœ… Tests intÃ©gration passent (8+ passed)
3. âœ… Documentation patterns d'intÃ©gration
4. âœ… Aucune rÃ©gression dans suite existante

### ğŸ¯ CritÃ¨res de succÃ¨s

- [ ] 8+ tests d'intÃ©gration crÃ©Ã©s
- [ ] Tous tests passent sans erreur
- [ ] Coverage intÃ©gration service/DB > 70%
- [ ] Aucune rÃ©gression dans suite existante
- [ ] Documentation claire des patterns

### ğŸ“š RÃ©fÃ©rences

**Fichiers Ã  Ã©tudier** :
- `services/lunar_interpretation_generator.py` : Service Ã  tester
- `models/lunar_interpretation.py` : ModÃ¨le DB temporelle
- `models/lunar_interpretation_template.py` : ModÃ¨le templates
- `tests/test_lunar_interpretation_generator.py` : Tests unitaires existants
- `tests/conftest.py` : Fixtures DB disponibles

**Pattern de test intÃ©gration** :
```python
@pytest.mark.asyncio
async def test_integration_scenario(db_session):
    # Setup DB state
    # Call service
    # Verify DB state changed
    # Cleanup
    pass
```

---

## ğŸ¤– Agent C - Task 4.2 : Benchmarks Performance (1h30)

### ğŸ“Š Contexte

Tu es l'Agent C de la Vague 4. Ta mission est de crÃ©er des benchmarks de performance pour mesurer et documenter les performances du systÃ¨me V2.

**PrÃ©requis complÃ©tÃ©s** :
- âœ… Routes API complÃ¨tes avec cache (Vague 3)
- âœ… Indexes DB existants (user_id, created_at, model_used)
- âœ… Cache applicatif (TTL 10min pour metadata)

### ğŸ¯ Objectif

Mesurer et documenter les performances du systÃ¨me pour identifier les optimisations futures et Ã©tablir des baselines.

### ğŸ“ TÃ¢ches principales

#### 1. CrÃ©er fichier `tests/test_lunar_performance.py` (45min)

**Benchmarks Ã  implÃ©menter** (minimum 6 benchmarks) :

**a) Benchmarks cache (2 benchmarks)** :
```python
@pytest.mark.asyncio
async def test_benchmark_metadata_cache_hit():
    """
    Mesurer temps de rÃ©ponse GET /metadata avec cache hit.

    Objectif : < 50ms
    """
    import time

    # Warmup cache
    # Mesurer 100 requÃªtes avec cache hit
    # Calculer moyenne, min, max, p95, p99
    # Assert moyenne < 50ms
    pass

@pytest.mark.asyncio
async def test_benchmark_metadata_cache_miss():
    """
    Mesurer temps de rÃ©ponse GET /metadata avec cache miss (calcul DB).

    Objectif : < 200ms
    """
    # Clear cache
    # Mesurer 10 requÃªtes avec cache miss
    # Calculer moyenne
    # Assert moyenne < 200ms
    pass
```

**b) Benchmarks gÃ©nÃ©ration (2 benchmarks)** :
```python
@pytest.mark.asyncio
async def test_benchmark_generation_db_cache_hit():
    """
    Mesurer temps gÃ©nÃ©ration avec cache DB temporelle.

    Objectif : < 100ms
    """
    # CrÃ©er LunarInterpretation en DB
    # Mesurer 50 gÃ©nÃ©rations (toutes cache hits)
    # Assert moyenne < 100ms
    pass

@pytest.mark.asyncio
async def test_benchmark_generation_template_fallback():
    """
    Mesurer temps gÃ©nÃ©ration avec fallback template.

    Objectif : < 300ms
    """
    # Mock Ã©chec Claude
    # Mesurer 20 gÃ©nÃ©rations (fallback template)
    # Assert moyenne < 300ms
    pass
```

**c) Benchmarks requÃªtes DB (2 benchmarks)** :
```python
@pytest.mark.asyncio
async def test_benchmark_db_query_with_indexes():
    """
    Mesurer performance requÃªtes SQL avec indexes.

    - COUNT(*) WHERE user_id = X
    - GROUP BY model_used
    - MAX(created_at) WHERE user_id = X

    Objectif : < 50ms par requÃªte
    """
    pass

@pytest.mark.asyncio
async def test_benchmark_concurrent_metadata_requests():
    """
    Mesurer performance avec 10 requÃªtes concurrentes.

    Objectif : < 500ms pour toutes (p95)
    """
    import asyncio

    # Lancer 10 requÃªtes en parallÃ¨le
    # Mesurer temps total
    # Assert p95 < 500ms
    pass
```

#### 2. CrÃ©er documentation `docs/PERFORMANCE_BENCHMARKS.md` (30min)

**Structure du document** :

```markdown
# Performance Benchmarks - Lunar Interpretation V2

**Date** : 2026-01-23
**Version** : V2 (Sprint 5 Vague 4)
**Environment** : Test local (MacBook M1, SQLite in-memory)

## ğŸ“Š RÃ©sultats Benchmarks

### Cache Performance

| Endpoint | ScÃ©nario | Moyenne | P95 | P99 | Objectif | Status |
|----------|----------|---------|-----|-----|----------|--------|
| GET /metadata | Cache hit | Xms | Xms | Xms | < 50ms | âœ… |
| GET /metadata | Cache miss | Xms | Xms | Xms | < 200ms | âœ… |

### GÃ©nÃ©ration Performance

| ScÃ©nario | Moyenne | P95 | P99 | Objectif | Status |
|----------|---------|-----|-----|----------|--------|
| DB cache hit | Xms | Xms | Xms | < 100ms | âœ… |
| Template fallback | Xms | Xms | Xms | < 300ms | âœ… |

### RequÃªtes DB Performance

| RequÃªte | Moyenne | Objectif | Status |
|---------|---------|----------|--------|
| COUNT user_id | Xms | < 50ms | âœ… |
| GROUP BY model_used | Xms | < 50ms | âœ… |
| MAX created_at | Xms | < 50ms | âœ… |

## ğŸ¯ Optimisations RecommandÃ©es

### Court terme
- [ ] Optimisation 1 (si benchmark Ã©choue)
- [ ] Optimisation 2

### Long terme
- [ ] Migration vers PostgreSQL production (vs SQLite test)
- [ ] Ajout index composite si nÃ©cessaire

## ğŸ“ˆ Evolution

| Date | Version | AmÃ©lioration | Note |
|------|---------|--------------|------|
| 2026-01-23 | V2 Initial | Baseline Ã©tablie | - |

## ğŸ” MÃ©thodologie

**Setup** :
- Environment : MacBook M1, 16GB RAM
- DB : SQLite in-memory (tests)
- Python : 3.10.11
- FastAPI : 0.109.0

**Mesures** :
- Moyenne : Mean de N requÃªtes
- P95 : 95th percentile
- P99 : 99th percentile

**DonnÃ©es de test** :
- 100 interprÃ©tations prÃ©-crÃ©Ã©es
- 5 utilisateurs diffÃ©rents
```

#### 3. Analyse et recommandations (15min)

**a) Analyser les rÃ©sultats** :
- Identifier les bottlenecks
- Comparer avec objectifs
- Noter les dÃ©viations

**b) Recommandations d'optimisation** :
```markdown
## Recommandations

### Si metadata cache miss > 200ms
- [ ] VÃ©rifier si indexes sont utilisÃ©s (EXPLAIN ANALYZE)
- [ ] ConsidÃ©rer index composite (user_id, created_at)

### Si gÃ©nÃ©ration template > 300ms
- [ ] Profiler lookup template
- [ ] Optimiser requÃªte SELECT template

### Si requÃªtes concurrentes > 500ms
- [ ] VÃ©rifier connection pool size
- [ ] Tester avec connection pooling optimisÃ©
```

### ğŸ“¦ Livrables

1. âœ… Fichier `tests/test_lunar_performance.py` avec 6+ benchmarks
2. âœ… Document `docs/PERFORMANCE_BENCHMARKS.md` avec rÃ©sultats
3. âœ… Recommandations d'optimisation documentÃ©es
4. âœ… Tous benchmarks exÃ©cutÃ©s avec succÃ¨s

### ğŸ¯ CritÃ¨res de succÃ¨s

- [ ] 6+ benchmarks crÃ©Ã©s
- [ ] Tous benchmarks exÃ©cutables (mÃªme si Ã©chouent)
- [ ] RÃ©sultats documentÃ©s dans PERFORMANCE_BENCHMARKS.md
- [ ] Recommandations d'optimisation claires
- [ ] Baselines Ã©tablies pour tracking futur

### ğŸ“š RÃ©fÃ©rences

**Fichiers Ã  Ã©tudier** :
- `routes/lunar.py` : Endpoints Ã  benchmarker
- `services/lunar_interpretation_generator.py` : Service gÃ©nÃ©ration
- `models/lunar_interpretation.py` : RequÃªtes DB Ã  mesurer
- `services/interpretation_cache_service.py` : Cache Ã  mesurer

**Libraries utiles** :
```python
import time
import asyncio
from statistics import mean, stdev
import pytest

# Exemple de benchmark
start = time.perf_counter()
# ... code Ã  mesurer
elapsed = time.perf_counter() - start
```

**Percentiles** :
```python
import numpy as np

times = [...]  # Liste de temps mesurÃ©s
p95 = np.percentile(times, 95)
p99 = np.percentile(times, 99)
```

---

## ğŸ“‹ Checklist Vague 4

### Agent A (Tests E2E)
- [ ] Fichier test_lunar_routes_e2e.py crÃ©Ã©
- [ ] 10+ tests E2E implÃ©mentÃ©s
- [ ] Tous tests passent
- [ ] Aucune rÃ©gression (pytest -q)
- [ ] Docstrings complÃ¨tes
- [ ] Commit: `test(api): ajouter tests E2E routes lunar V2`

### Agent B (Tests IntÃ©gration)
- [ ] Fichier test_lunar_integration.py crÃ©Ã©
- [ ] 8+ tests intÃ©gration implÃ©mentÃ©s
- [ ] Tous tests passent
- [ ] Patterns d'intÃ©gration documentÃ©s
- [ ] Aucune rÃ©gression (pytest -q)
- [ ] Commit: `test(api): ajouter tests intÃ©gration service/DB V2`

### Agent C (Benchmarks Performance)
- [ ] Fichier test_lunar_performance.py crÃ©Ã©
- [ ] 6+ benchmarks implÃ©mentÃ©s
- [ ] Document PERFORMANCE_BENCHMARKS.md crÃ©Ã©
- [ ] RÃ©sultats documentÃ©s
- [ ] Recommandations rÃ©digÃ©es
- [ ] Commit: `perf(api): ajouter benchmarks performance V2`

---

## ğŸ¯ Coordination Inter-Agents

**Ordre d'exÃ©cution recommandÃ©** :
1. **ParallÃ¨le** : Les 3 agents peuvent travailler en parallÃ¨le (pas de dÃ©pendances)
2. **Review croisÃ©e** : Chaque agent peut relire les tests des autres
3. **Validation finale** : Un agent valide que pytest -q passe pour tous

**Communication** :
- Si un agent dÃ©couvre un bug, le signaler immÃ©diatement
- Si un test Ã©choue, dÃ©bugger avant de passer Ã  la suite
- Partager les patterns de test utiles

**Validation finale** (aprÃ¨s les 3 agents) :
```bash
# ExÃ©cuter toute la suite
pytest -v

# Objectif : 530+ tests passed (514 + 16 nouveaux minimum)
# VÃ©rifier : 0 failed
```

---

## ğŸ“š Ressources Communes

**Documentation Ã  consulter** :
- `.claude/CLAUDE.md` : Ã‰tat du projet
- `docs/MIGRATION_PLAN.md` : Plan migration V2
- `docs/LUNAR_ARCHITECTURE_V2.md` : Architecture V2

**Commandes utiles** :
```bash
# Lancer tests d'un agent spÃ©cifique
pytest tests/test_lunar_routes_e2e.py -v
pytest tests/test_lunar_integration.py -v
pytest tests/test_lunar_performance.py -v

# Coverage pour un fichier
pytest tests/test_lunar_routes_e2e.py --cov=routes.lunar --cov-report=term

# Profiling performance
pytest tests/test_lunar_performance.py --durations=10
```

**Fixtures disponibles** (conftest.py) :
- `override_dependencies` : Override auth + DB
- `fake_user` : Mock User
- `FakeAsyncSession` : Mock DB session

---

**Bonne chance aux 3 agents ! ğŸš€**

La Vague 4 est critique pour garantir la qualitÃ© et la performance du systÃ¨me V2 avant la mise en production.
